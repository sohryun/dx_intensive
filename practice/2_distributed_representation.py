# -*- coding: utf-8 -*-
"""자연어처리_2_Distributed_representation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/KU-DIC/LG_natural_language_processing_day21/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_2_Distributed_representation.ipynb

## [LG 전자] 자연어 처리 # 2 : 분상 표상 기반의 임베딩 기법
* Word2Vec
* GloVe
* FastText

예상 난이도 ⭐️

## 실습 요약
* 분상표상 기반의 임베딩 기법을 학습합니다
* 여러가지 임베딩 기법을 통해 간단한 모델을 구축한 후 성능을 비교합니다

------

### STEP 0. 환경 구축하기
* 필요한 library들을 import 합니다
"""

import random
import numpy as np
import pandas as pd
from time import time
from tqdm import tqdm
import matplotlib.pyplot as plt
plt.rcParams['axes.unicode_minus'] = False
#%matplotlib inline #생성한 figure를 notebook에서 볼 수있게 해주는 코드

import gensim
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

#check torch version & device
print ("PyTorch version:[%s]."%(torch.__version__))
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print ("device:[%s]."%(device))

! pip install konlpy -q
! pip install glove_python_binary -q

# set random seed 

def set_seed(random_seed):
    torch.manual_seed(random_seed)
    torch.cuda.manual_seed(random_seed)
    torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    np.random.seed(random_seed)
    random.seed(random_seed)
    
random_seed = 42
set_seed(random_seed)

"""### STEP 1. 데이터 준비하기
금일 실습에서는 **Naver_shopping**에서 수집된 **리뷰 데이터**를 활용합니다.
* 제품별 후기를 별점과 함께 수집한 데이터
* 데이터셋 출처

  * 1) https://github.com/songys/AwesomeKorean_Data
  * 2) https://github.com/bab2min/corpus/tree/master/sentiment


* 해당 실습에서는 전체 데이터 중 50%인 10만개의 데이터를 사용함 (긍정 : 50000개, 부정 : 50000개)



"""

# github에서 데이터 불러오기
!git clone https://github.com/KU-DIC/LG_natural_language_processing_day21

# 데이터셋 읽기
with open('./LG_natural_language_processing_day21/data/naver_shopping.txt','r',encoding='utf-8') as f:
  data = f.readlines()

data[0]

# 분석에 사용할 형태로 가공하기
rate = []
label = []
sentence = []
for cur_review in tqdm(data):
  # 평점과 리뷰 문장 분리
  cur_review = cur_review.split('\t')
  # 평점
  rate.append(cur_review[0])
  # 리뷰 문장
  sentence.append(cur_review[1].strip('\n'))
  # 평점 4,5 : Positive, 
  # 평점 1,2,3 : Negative
  if int(cur_review[0]) >3:
    label.append(1) # positive
  else:
    label.append(-1) # negative

# 데이터 프레임 형태로 변환하기
df = {
    "rate" : rate,
    "label" : label,
    "sentence" : sentence
}
df = pd.DataFrame(df)
df = df.sample(frac=0.5,replace=False, random_state=42)

df.shape

df.head()

df['label'].hist()

df.head()

df['sentence'].head()

"""### STEP 2. 데이터 전처리 (Preprocessing)"""

review_sentences = df['sentence'].to_list()

import re
def preprocess(text):
  text = re.sub('[-=+,#/\?:^$~@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…》]','', text)
  text = re.sub('[ㅠㅎㅋ]','', text)
  return text

normalize_sentence = []
for sentence in tqdm(review_sentences):
  sentence = preprocess(sentence)
  normalize_sentence.append(sentence)

df['normalize_sentence'] = normalize_sentence

df

! pip install konlpy -q

"""### STEP 3. 토큰화 진행 (Tokenization)"""

! pip install konlpy -q

# Okt(Open Korea Text)
from konlpy.tag import Okt  
okt=Okt() 

print(okt.morphs(normalize_sentence[0]))

tokenized_sentence = []
for sent in tqdm(normalize_sentence):
  sent = okt.morphs(sent)
  tokenized_sentence.append(sent)

df['tokenized_sentence'] = tokenized_sentence

df.head()

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size =0.2, random_state= 42)
train_df['label'].hist()
test_df['label'].hist()

"""### STEP 4. 벡터화 진행 (Vectorization)

#### Method 1. Word2Vec
"""

from gensim.models import Word2Vec

train_sentence = train_df['tokenized_sentence'].to_list()
test_sentence = test_df['tokenized_sentence'].to_list()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = Word2Vec(
#     sentences=train_sentence,
#     size=100, 
#     window=5, 
#     min_count=1, 
#     workers=4,
#     seed=random_seed)

model.build_vocab([['<UNK>']],update=True)

# model.save("word2vec.model")

vocab = list(model.wv.vocab)

len(vocab),type(vocab)

model.wv.most_similar('별로', topn=10)

train_sentence_vector = []
for cur_sent in tqdm(train_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += model.wv[word]
  train_sentence_vector.append(vector)

test_sentence_vector = []
for cur_sent in tqdm(test_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += model.wv[word]
  test_sentence_vector.append(vector)

train_df['w2v_sentence_vector'] = train_sentence_vector
test_df['w2v_sentence_vector'] = test_sentence_vector

"""#### Method 2. Glove"""

from glove import Corpus, Glove

corpus = Corpus() 

# 훈련 데이터로부터 GloVe에서 사용할 동시 등장 행렬 생성
corpus.fit(train_sentence, window=5)
glove = Glove(no_components=100, learning_rate=0.05, random_state=42)

# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.
glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)
glove.add_dictionary(corpus.dictionary)

glove.most_similar("별로",number=10)

vocab = []
for word, word_idx in tqdm(glove.dictionary.items()):
  vocab.append(word)

train_sentence_vector = []
for cur_sent in tqdm(train_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += glove.word_vectors[glove.dictionary[word]]
  train_sentence_vector.append(vector)

test_sentence_vector = []
for cur_sent in tqdm(test_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += glove.word_vectors[glove.dictionary[word]]
  test_sentence_vector.append(vector)

train_df['glove_sentence_vector'] = train_sentence_vector
test_df['glove_sentence_vector'] = test_sentence_vector

"""#### Method 3. FastText"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from gensim.models import FastText
# 
# model = FastText(
#     sentences = tokenized_sentence,
#     size=100,
#     window=5,
#     min_count=1,
#     workers=4)

vocab = list(model.wv.vocab)

len(vocab)

model.wv.most_similar('별로', topn=10)

train_sentence_vector = []
for cur_sent in tqdm(train_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += model.wv[word]
  train_sentence_vector.append(vector)

test_sentence_vector = []
for cur_sent in tqdm(test_sentence):
  vector = np.zeros(100,)
  for word in cur_sent:
    if word in vocab:
      vector += model.wv[word]
  test_sentence_vector.append(vector)

train_df['ft_sentence_vector'] = train_sentence_vector
test_df['ft_sentence_vector'] = test_sentence_vector

# train_df.to_csv('./train_df.csv')
# test_df.to_csv('./test_df.csv')

"""### STEP 5. 모델 구축하기 (Modeling)
* Word2Vec, Glove, FastText를 활용하여 감성분석 모델 만들기

#### Word2Vec을 사용하여 감성 분석하기
"""

w2v_df = train_df[['label','w2v_sentence_vector']]

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(random_state=42)

sent_vec = np.stack(w2v_df.w2v_sentence_vector,axis=0)
label_vec = np.array(w2v_df.label.to_list()).reshape(-1,)

model = model.fit(sent_vec, label_vec)
model.score(sent_vec, label_vec)

w2v_df = test_df[['label','w2v_sentence_vector']]
sent_vec = np.stack(w2v_df.w2v_sentence_vector,axis=0)
label_vec = np.array(w2v_df.label.to_list()).reshape(-1,)
model.score(sent_vec, label_vec)

glove_df = train_df[['label','glove_sentence_vector']]

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(random_state=42)

sent_vec = np.stack(glove_df.glove_sentence_vector,axis=0)
label_vec = np.array(glove_df.label.to_list()).reshape(-1,)

model = model.fit(sent_vec, label_vec)
model.score(sent_vec, label_vec)

glove_df = test_df[['label','glove_sentence_vector']]

sent_vec = np.stack(glove_df.glove_sentence_vector,axis=0)
label_vec = np.array(glove_df.label.to_list()).reshape(-1,)
model.score(sent_vec, label_vec)

from sklearn.tree import DecisionTreeClassifier
ft_df = train_df[['label','ft_sentence_vector']]

model = DecisionTreeClassifier(random_state=42)

sent_vec = np.stack(ft_df.ft_sentence_vector,axis=0)
label_vec = np.array(ft_df.label.to_list()).reshape(-1,)

model = model.fit(sent_vec, label_vec)
model.score(sent_vec, label_vec)

ft_df = test_df[['label','ft_sentence_vector']]

sent_vec = np.stack(ft_df.ft_sentence_vector,axis=0)
label_vec = np.array(ft_df.label.to_list()).reshape(-1,)
model.score(sent_vec, label_vec)
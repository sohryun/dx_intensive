{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64d3011-7da0-4488-9118-128a7672da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "326940c0-4514-42c4-ac97-23b15a45c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # Detect if we have a GPU available\n",
    "\n",
    "# seed 고정\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c8f00f7-87cd-477c-802b-bc453c6f1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, raw_data, window= 10,stride=1,mode='train'):\n",
    "        self.raw_data = raw_data\n",
    "        self.stride = stride\n",
    "        self.window = window \n",
    "        self.mode = mode\n",
    "        #raw_data = data\n",
    "        y = raw_data.iloc[:,-1]\n",
    "        raw_data = raw_data.iloc[:,:-2]\n",
    "        \n",
    "        data = torch.from_numpy(raw_data.values)\n",
    "        \n",
    "        data_y = torch.from_numpy(y.values)\n",
    "        #data = torch.tensor(data).double()\n",
    "        \n",
    "        self.x, self.y = self.process(data,data_y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def process(self,data,data_y) : \n",
    "        x_arr, y_arr = [], []\n",
    "\n",
    "        is_train = self.mode == 'train'\n",
    "        #is_train = True\n",
    "        slide_win = self.window\n",
    "        stride_win = self.stride\n",
    "        total_time_len = len(data)\n",
    "        \n",
    "        rang = range(slide_win,total_time_len,stride_win) if is_train else range(slide_win,total_time_len)\n",
    "        \n",
    "        for i in rang :\n",
    "#            i = 10\n",
    "            ft = data[i-slide_win:i,:]\n",
    "            x_arr.append(ft)\n",
    "            \n",
    "            label = data_y[i-slide_win:i]\n",
    "            y_arr.append(label)\n",
    "       \n",
    "        x = torch.stack(x_arr).contiguous()\n",
    "        y = torch.stack(y_arr).contiguous()\n",
    "        \n",
    "        \n",
    "        return x , y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx].float()\n",
    "        label = self.y[idx].float()\n",
    "        ## last value return\n",
    "        feature = torch.transpose(feature,0,1)\n",
    "        label = label[-1]\n",
    "        return feature , label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a06417b-d848-4c01-a544-36fc77a55135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb97bbfb-463d-40c3-a1af-b05ab990cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./230216_RawDataSet.csv', index_col=0)\n",
    "df.columns = ['inlet','outlet','g','w']\n",
    "#df['before'] = df['w'].shift(1)\n",
    "#df['before'].iloc[0] = df['w'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af49f47-9791-42a1-bd59-56a457277e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(0.8 * len(df))\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c5fe4d-ab9c-42ae-bed9-fbe97f4b70e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_valid = int(0.9 * len(df))\n",
    "n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f268541-d242-4fe5-8b16-f0f9d9cdae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:n_train,:]\n",
    "val_df = df.iloc[n_train:n_valid,:]\n",
    "test_df = df.iloc[n_valid:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3efd47b5-7efb-4a2d-91f2-a6184a46f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeDataset(train_df,window=10)\n",
    "val_dataset = TimeDataset(val_df,window=10)\n",
    "test_dataset = TimeDataset(test_df,window=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05e01ad-f913-4c43-b888-efeb997c7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccf4e9de-5e77-46ce-9e2f-48dd5f7c4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=64,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "212905a6-24d0-412c-bc3d-20c01ee3ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#shape: (#실험자, dim 561, len 281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "975ebf04-d25c-41fa-bdac-55e17ff0db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-dimensional convolution layer로 구성된 CNN 모델\n",
    "# 2개의 1-dimensional convolution layer와 1개의 fully-connected layer로 구성되어 있음\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        # 첫 번째 1-dimensional convolution layer 구축\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(2, 8, kernel_size=4),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # 두 번째 1-dimensional convolution layer 구축\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=2),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # fully-connected layer 구축\n",
    "        self.fc = nn.Linear(16 * 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a025b24-8edb-4941-9001-f94b13ba2c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(2, 8, kernel_size=(4,), stride=(1,))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(8, 16, kernel_size=(2,), stride=(1,))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 구축\n",
    "device = 'cuda'\n",
    "model = CNN_1D(num_classes=1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5759381-2749-4e6e-b80d-fb82117255dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer 구축하기\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f66b02d-e306-4cd6-93dd-4a41b9da0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4a44bdc-a3c9-4767-bbf6-1a7cd2e9fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    val_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() \n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += labels.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "            #epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'{epoch} : save_model')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0a6a294-f0e4-4995-9d16-ac7487bd71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6bd5e5c-8e8a-4487-b4fe-af6123278cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function 설정\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b9cc89e-9001-4ceb-bac4-09c9398422b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2237\n",
      "val Loss: 0.0932\n",
      "0 : save_model\n",
      "\n",
      "Epoch 2/500\n",
      "----------\n",
      "train Loss: 0.1926\n",
      "val Loss: 0.0734\n",
      "1 : save_model\n",
      "\n",
      "Epoch 3/500\n",
      "----------\n",
      "train Loss: 0.1740\n",
      "val Loss: 0.0564\n",
      "2 : save_model\n",
      "\n",
      "Epoch 4/500\n",
      "----------\n",
      "train Loss: 0.1554\n",
      "val Loss: 0.0418\n",
      "3 : save_model\n",
      "\n",
      "Epoch 5/500\n",
      "----------\n",
      "train Loss: 0.1317\n",
      "val Loss: 0.0296\n",
      "4 : save_model\n",
      "\n",
      "Epoch 6/500\n",
      "----------\n",
      "train Loss: 0.1154\n",
      "val Loss: 0.0198\n",
      "5 : save_model\n",
      "\n",
      "Epoch 7/500\n",
      "----------\n",
      "train Loss: 0.1001\n",
      "val Loss: 0.0124\n",
      "6 : save_model\n",
      "\n",
      "Epoch 8/500\n",
      "----------\n",
      "train Loss: 0.0878\n",
      "val Loss: 0.0069\n",
      "7 : save_model\n",
      "\n",
      "Epoch 9/500\n",
      "----------\n",
      "train Loss: 0.0774\n",
      "val Loss: 0.0033\n",
      "8 : save_model\n",
      "\n",
      "Epoch 10/500\n",
      "----------\n",
      "train Loss: 0.0654\n",
      "val Loss: 0.0011\n",
      "9 : save_model\n",
      "\n",
      "Epoch 11/500\n",
      "----------\n",
      "train Loss: 0.0582\n",
      "val Loss: 0.0003\n",
      "10 : save_model\n",
      "\n",
      "Epoch 12/500\n",
      "----------\n",
      "train Loss: 0.0505\n",
      "val Loss: 0.0005\n",
      "\n",
      "Epoch 13/500\n",
      "----------\n",
      "train Loss: 0.0497\n",
      "val Loss: 0.0015\n",
      "\n",
      "Epoch 14/500\n",
      "----------\n",
      "train Loss: 0.0420\n",
      "val Loss: 0.0031\n",
      "\n",
      "Epoch 15/500\n",
      "----------\n",
      "train Loss: 0.0406\n",
      "val Loss: 0.0051\n",
      "\n",
      "Epoch 16/500\n",
      "----------\n",
      "train Loss: 0.0361\n",
      "val Loss: 0.0075\n",
      "\n",
      "Epoch 17/500\n",
      "----------\n",
      "train Loss: 0.0329\n",
      "val Loss: 0.0101\n",
      "\n",
      "Epoch 18/500\n",
      "----------\n",
      "train Loss: 0.0324\n",
      "val Loss: 0.0126\n",
      "\n",
      "Epoch 19/500\n",
      "----------\n",
      "train Loss: 0.0298\n",
      "val Loss: 0.0152\n",
      "\n",
      "Epoch 20/500\n",
      "----------\n",
      "train Loss: 0.0281\n",
      "val Loss: 0.0178\n",
      "\n",
      "Epoch 21/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0203\n",
      "\n",
      "Epoch 22/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0226\n",
      "\n",
      "Epoch 23/500\n",
      "----------\n",
      "train Loss: 0.0273\n",
      "val Loss: 0.0246\n",
      "\n",
      "Epoch 24/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0266\n",
      "\n",
      "Epoch 25/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0282\n",
      "\n",
      "Epoch 26/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0298\n",
      "\n",
      "Epoch 27/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0313\n",
      "\n",
      "Epoch 28/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0327\n",
      "\n",
      "Epoch 29/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0338\n",
      "\n",
      "Epoch 30/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0351\n",
      "\n",
      "Epoch 31/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0360\n",
      "\n",
      "Epoch 32/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0368\n",
      "\n",
      "Epoch 33/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0375\n",
      "\n",
      "Epoch 34/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0381\n",
      "\n",
      "Epoch 35/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 36/500\n",
      "----------\n",
      "train Loss: 0.0223\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 37/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 38/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 39/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 40/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 41/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 42/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 43/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 44/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 45/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 46/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 47/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 48/500\n",
      "----------\n",
      "train Loss: 0.0225\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 49/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 50/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 51/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 52/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 53/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 54/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 55/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 56/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 57/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 58/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 59/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 60/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 61/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 62/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 63/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 64/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 65/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 66/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 67/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 68/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 69/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 70/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 71/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 72/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 73/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 74/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 75/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 76/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 77/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 78/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 79/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 80/500\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 81/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 82/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 83/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 84/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 85/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 86/500\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 87/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 88/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 89/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 90/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 91/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 92/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 93/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 94/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 95/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 96/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 97/500\n",
      "----------\n",
      "train Loss: 0.0273\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 98/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 99/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 100/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 101/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 102/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 103/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 104/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 105/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 106/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 107/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 108/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 109/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 110/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 111/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 112/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 113/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 114/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 115/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 116/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 117/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 118/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 119/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 120/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 121/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 122/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 123/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 124/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 125/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 126/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 127/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 128/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 129/500\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 130/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 131/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 132/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 133/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 134/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 135/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 136/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 137/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 138/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 139/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 140/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 141/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 142/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 143/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 144/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 145/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 146/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 147/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 148/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 149/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 150/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 151/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 152/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 153/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 154/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 155/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 156/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 157/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 158/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 159/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 160/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 161/500\n",
      "----------\n",
      "train Loss: 0.0223\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 162/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 163/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 164/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 165/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 166/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 167/500\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 168/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 169/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 170/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 171/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 172/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 173/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 174/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 175/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 176/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 177/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 178/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 179/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 180/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 181/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 182/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 183/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 184/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 185/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 186/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 187/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 188/500\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 189/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 190/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 191/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 192/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 193/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 194/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 195/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 196/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 197/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 198/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 199/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 200/500\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 201/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 202/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 203/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 204/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 205/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 206/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 207/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 208/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 209/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 210/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 211/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 212/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 213/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 214/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 215/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 216/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 217/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 218/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 219/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 220/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 221/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 222/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 223/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 224/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 225/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 226/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 227/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 228/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 229/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 230/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 231/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 232/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 233/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 234/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 235/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 236/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 237/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 238/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 239/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0422\n",
      "\n",
      "Epoch 240/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 241/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 242/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 243/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 244/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 245/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 246/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 247/500\n",
      "----------\n",
      "train Loss: 0.0265\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 248/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 249/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 250/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 251/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 252/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 253/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 254/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 255/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 256/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 257/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 258/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 259/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 260/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 261/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 262/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 263/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 264/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 265/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 266/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 267/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 268/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 269/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 270/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 271/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 272/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 273/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 274/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 275/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 276/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 277/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 278/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 279/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 280/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 281/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 282/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 283/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 284/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 285/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 286/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 287/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 288/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 289/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 290/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 291/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 292/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 293/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 294/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 295/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 296/500\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "val Loss: 0.0426\n",
      "\n",
      "Epoch 297/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0426\n",
      "\n",
      "Epoch 298/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0422\n",
      "\n",
      "Epoch 299/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0421\n",
      "\n",
      "Epoch 300/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 301/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 302/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 303/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 304/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 305/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 306/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 307/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 308/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 309/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 310/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 311/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 312/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 313/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 314/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 315/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 316/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 317/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 318/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 319/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 320/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 321/500\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 322/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 323/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0424\n",
      "\n",
      "Epoch 324/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0429\n",
      "\n",
      "Epoch 325/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 326/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 327/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 328/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 329/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 330/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 331/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 332/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 333/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 334/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 335/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 336/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 337/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 338/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0421\n",
      "\n",
      "Epoch 339/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0422\n",
      "\n",
      "Epoch 340/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 341/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 342/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 343/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 344/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 345/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 346/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 347/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 348/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 349/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 350/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 351/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 352/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 353/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 354/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0376\n",
      "\n",
      "Epoch 355/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0367\n",
      "\n",
      "Epoch 356/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0377\n",
      "\n",
      "Epoch 357/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0380\n",
      "\n",
      "Epoch 358/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 359/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 360/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 361/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 362/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 363/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 364/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 365/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 366/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 367/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 368/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 369/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0423\n",
      "\n",
      "Epoch 370/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0429\n",
      "\n",
      "Epoch 371/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0428\n",
      "\n",
      "Epoch 372/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 373/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 374/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 375/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 376/500\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 377/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 378/500\n",
      "----------\n",
      "train Loss: 0.0265\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 379/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 380/500\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "val Loss: 0.0433\n",
      "\n",
      "Epoch 381/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0426\n",
      "\n",
      "Epoch 382/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 383/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 384/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 385/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 386/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 387/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 388/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 389/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 390/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 391/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 392/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 393/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 394/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 395/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 396/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 397/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 398/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 399/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 400/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 401/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 402/500\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 403/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 404/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0427\n",
      "\n",
      "Epoch 405/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 406/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 407/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 408/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 409/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0378\n",
      "\n",
      "Epoch 410/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0373\n",
      "\n",
      "Epoch 411/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 412/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 413/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 414/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 415/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 416/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 417/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 418/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 419/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 420/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 421/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 422/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 423/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 424/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 425/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 426/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 427/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 428/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 429/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 430/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 431/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 432/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 433/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0381\n",
      "\n",
      "Epoch 434/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0377\n",
      "\n",
      "Epoch 435/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 436/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 437/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 438/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 439/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 440/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 441/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0378\n",
      "\n",
      "Epoch 442/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0378\n",
      "\n",
      "Epoch 443/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 444/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 445/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 446/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0425\n",
      "\n",
      "Epoch 447/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 448/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 449/500\n",
      "----------\n",
      "train Loss: 0.0224\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 450/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 451/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 452/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 453/500\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 454/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 455/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 456/500\n",
      "----------\n",
      "train Loss: 0.0264\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 457/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0434\n",
      "\n",
      "Epoch 458/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 459/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 460/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 461/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 462/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 463/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 464/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 465/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 466/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0382\n",
      "\n",
      "Epoch 467/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 468/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 469/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 470/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 471/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 472/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 473/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 474/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 475/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 476/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 477/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 478/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 479/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 480/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 481/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0369\n",
      "\n",
      "Epoch 482/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0374\n",
      "\n",
      "Epoch 483/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 484/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 485/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 486/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 487/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 488/500\n",
      "----------\n",
      "train Loss: 0.0223\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 489/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 490/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 491/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 492/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 493/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 494/500\n",
      "----------\n",
      "train Loss: 0.0224\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 495/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 496/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0380\n",
      "\n",
      "Epoch 497/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0380\n",
      "\n",
      "Epoch 498/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0382\n",
      "\n",
      "Epoch 499/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 500/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0421\n",
      "\n",
      "Training complete in 0m 16s\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model, val_acc_history = train_model(model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84d5195e-5625-42b9-ab2d-f2793a04a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_1D(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(2, 8, kernel_size=(4,), stride=(1,))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1d(8, 16, kernel_size=(2,), stride=(1,))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e0cd68e-271e-4c7d-ba0d-1cf3ef256c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,criterion, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "    \n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        #corrects = 0\n",
    "        total = 0\n",
    "        label_list = []\n",
    "        output_list = [] \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy().tolist()\n",
    "            output_list.extend(outputs)\n",
    "            label_list.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "            \n",
    "    return output_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46fd6e15-493d-496b-9d99-c8966172cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list,label_list = test_model(model, criterion, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d4ac77b-8f41-48b9-b8f6-77b16f87eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225],\n",
       " [2.4514100551605225]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7074db2d-79e8-4ad4-ab32-1edbba2b7f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.304606,\n",
       " 2.396678,\n",
       " 2.466772,\n",
       " 2.3624077,\n",
       " 2.3153226,\n",
       " 2.307558,\n",
       " 2.4113579,\n",
       " 2.4476678,\n",
       " 2.4475827,\n",
       " 2.2919137,\n",
       " 2.2839155,\n",
       " 2.30857,\n",
       " 2.431437,\n",
       " 2.3871229,\n",
       " 2.3663397,\n",
       " 2.4650793,\n",
       " 2.4962463,\n",
       " 2.5044162,\n",
       " 2.4053051,\n",
       " 2.4317226,\n",
       " 2.384631,\n",
       " 2.4047058,\n",
       " 2.3639357,\n",
       " 2.4228513,\n",
       " 2.5234191,\n",
       " 2.349637,\n",
       " 2.430969,\n",
       " 2.4822721,\n",
       " 2.47304,\n",
       " 2.435214,\n",
       " 2.3145738,\n",
       " 2.440728,\n",
       " 2.2672696,\n",
       " 2.2844753,\n",
       " 2.4609153,\n",
       " 2.5124917,\n",
       " 2.4430888,\n",
       " 2.5957258,\n",
       " 2.4064314,\n",
       " 2.1980708,\n",
       " 2.4664056,\n",
       " 2.375425]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91c535b-7dcd-41c8-b538-db3e900b2f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cd3ac9e190>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(output_list)\n",
    "plt.plot(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4399198d-15e7-4aa5-adcb-202927f07572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1c6a8b7-732f-4ca8-b268-0a8820b427cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07270169258117676"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(output_list,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "151eeef1-f677-4533-be8a-9f34e6710b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008976688485055158"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(output_list,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18a021-e3d7-47f3-bdf5-46110f596505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463938a-1865-4e6c-85c0-d1b8187c6cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64d3011-7da0-4488-9118-128a7672da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8f00f7-87cd-477c-802b-bc453c6f1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch\n",
    "\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, raw_data, window= 10,stride=1,mode='train'):\n",
    "        self.raw_data = raw_data\n",
    "        self.stride = stride\n",
    "        self.window = window \n",
    "        self.mode = mode\n",
    "        #raw_data = data\n",
    "        y = raw_data.iloc[:,-1]\n",
    "        raw_data = raw_data.iloc[:,:-2]\n",
    "        \n",
    "        data = torch.from_numpy(raw_data.values)\n",
    "        \n",
    "        data_y = torch.from_numpy(y.values)\n",
    "        #data = torch.tensor(data).double()\n",
    "        \n",
    "        self.x, self.y = self.process(data,data_y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def process(self,data,data_y) : \n",
    "        x_arr, y_arr = [], []\n",
    "\n",
    "        is_train = self.mode == 'train'\n",
    "        #is_train = True\n",
    "        slide_win = self.window\n",
    "        stride_win = self.stride\n",
    "        total_time_len = len(data)\n",
    "        \n",
    "        rang = range(slide_win,total_time_len,stride_win) if is_train else range(slide_win,total_time_len)\n",
    "        \n",
    "        for i in rang :\n",
    "#            i = 10\n",
    "            ft = data[i-slide_win:i,:]\n",
    "            x_arr.append(ft)\n",
    "            \n",
    "            label = data_y[i-slide_win:i]\n",
    "            y_arr.append(label)\n",
    "       \n",
    "        x = torch.stack(x_arr).contiguous()\n",
    "        y = torch.stack(y_arr).contiguous()\n",
    "        \n",
    "        \n",
    "        return x , y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx].float()\n",
    "        label = self.y[idx].float()\n",
    "        ## last value return\n",
    "        feature = torch.transpose(feature,0,1)\n",
    "        label = label[-1]\n",
    "        return feature , label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a06417b-d848-4c01-a544-36fc77a55135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb97bbfb-463d-40c3-a1af-b05ab990cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./230216_RawDataSet.csv', index_col=0)\n",
    "df.columns = ['inlet','outlet','g','w']\n",
    "#df['before'] = df['w'].shift(1)\n",
    "#df['before'].iloc[0] = df['w'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af49f47-9791-42a1-bd59-56a457277e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = int(0.8 * len(df))\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0c5fe4d-ab9c-42ae-bed9-fbe97f4b70e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "459"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_valid = int(0.9 * len(df))\n",
    "n_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f268541-d242-4fe5-8b16-f0f9d9cdae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:n_train,:]\n",
    "val_df = df.iloc[n_train:n_valid,:]\n",
    "test_df = df.iloc[n_valid:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efd47b5-7efb-4a2d-91f2-a6184a46f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeDataset(train_df,window=10)\n",
    "val_dataset = TimeDataset(val_df,window=10)\n",
    "test_dataset = TimeDataset(test_df,window=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c05e01ad-f913-4c43-b888-efeb997c7a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf4e9de-5e77-46ce-9e2f-48dd5f7c4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset,batch_size=64,shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=64,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "212905a6-24d0-412c-bc3d-20c01ee3ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#shape: (#실험자, dim 561, len 281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "975ebf04-d25c-41fa-bdac-55e17ff0db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-dimensional convolution layer로 구성된 CNN 모델\n",
    "# 2개의 1-dimensional convolution layer와 1개의 fully-connected layer로 구성되어 있음\n",
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        # 첫 번째 1-dimensional convolution layer 구축\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(2, 8, kernel_size=4),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # 두 번째 1-dimensional convolution layer 구축\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(8, 16, kernel_size=2),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool1d(2)\n",
    "        )\n",
    "        # fully-connected layer 구축\n",
    "        self.fc = nn.Linear(16 * 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a025b24-8edb-4941-9001-f94b13ba2c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv1d(2, 8, kernel_size=(4,), stride=(1,))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv1d(8, 16, kernel_size=(2,), stride=(1,))\n",
      "    (1): Tanh()\n",
      "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "  )\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 1D CNN 구축\n",
    "device = 'cuda'\n",
    "model = CNN_1D(num_classes=1)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5759381-2749-4e6e-b80d-fb82117255dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD optimizer 구축하기\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f66b02d-e306-4cd6-93dd-4a41b9da0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a44bdc-a3c9-4767-bbf6-1a7cd2e9fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, num_epochs, optimizer):\n",
    "    since = time.time()\n",
    "\n",
    "    val_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1000000.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # 각 epoch마다 순서대로 training과 validation을 진행\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 training mode로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 validation mode로 설정\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_total = 0\n",
    "\n",
    "            # training과 validation 단계에 맞는 dataloader에 대하여 학습/검증 진행\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # parameter gradients를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # training 단계에서만 gradient 업데이트 수행\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # input을 model에 넣어 output을 도출한 후, loss를 계산함\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # output 중 최댓값의 위치에 해당하는 class로 예측을 수행\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward (optimize): training 단계에서만 수행\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # batch별 loss를 축적함\n",
    "                running_loss += loss.item() \n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "                running_total += labels.size(0)\n",
    "\n",
    "            # epoch의 loss 및 accuracy 도출\n",
    "            epoch_loss = running_loss / running_total\n",
    "            #epoch_acc = running_corrects.double() / running_total\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            # validation 단계에서 validation loss가 감소할 때마다 best model 가중치를 업데이트함\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(f'{epoch} : save_model')\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_history.append(epoch_loss)\n",
    "\n",
    "        print()\n",
    "\n",
    "    # 전체 학습 시간 계산\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    # validation loss가 가장 낮았을 때의 best model 가중치를 불러와 best model을 구축함\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # best model 가중치 저장\n",
    "    # torch.save(best_model_wts, '../output/best_model.pt')\n",
    "    return model, val_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0a6a294-f0e4-4995-9d16-ac7487bd71bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trining 단계에서 사용할 Dataloader dictionary 생성\n",
    "dataloaders_dict = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6bd5e5c-8e8a-4487-b4fe-af6123278cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function 설정\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b9cc89e-9001-4ceb-bac4-09c9398422b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([14])) that is different to the input size (torch.Size([14, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.2183\n",
      "val Loss: 0.0829\n",
      "0 : save_model\n",
      "\n",
      "Epoch 2/500\n",
      "----------\n",
      "train Loss: 0.1922\n",
      "val Loss: 0.0623\n",
      "1 : save_model\n",
      "\n",
      "Epoch 3/500\n",
      "----------\n",
      "train Loss: 0.1581\n",
      "val Loss: 0.0455\n",
      "2 : save_model\n",
      "\n",
      "Epoch 4/500\n",
      "----------\n",
      "train Loss: 0.1429\n",
      "val Loss: 0.0293\n",
      "3 : save_model\n",
      "\n",
      "Epoch 5/500\n",
      "----------\n",
      "train Loss: 0.1190\n",
      "val Loss: 0.0208\n",
      "4 : save_model\n",
      "\n",
      "Epoch 6/500\n",
      "----------\n",
      "train Loss: 0.0982\n",
      "val Loss: 0.0130\n",
      "5 : save_model\n",
      "\n",
      "Epoch 7/500\n",
      "----------\n",
      "train Loss: 0.0890\n",
      "val Loss: 0.0074\n",
      "6 : save_model\n",
      "\n",
      "Epoch 8/500\n",
      "----------\n",
      "train Loss: 0.0777\n",
      "val Loss: 0.0035\n",
      "7 : save_model\n",
      "\n",
      "Epoch 9/500\n",
      "----------\n",
      "train Loss: 0.0675\n",
      "val Loss: 0.0013\n",
      "8 : save_model\n",
      "\n",
      "Epoch 10/500\n",
      "----------\n",
      "train Loss: 0.0566\n",
      "val Loss: 0.0004\n",
      "9 : save_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lee\\anaconda3\\envs\\LG_DIC\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([41])) that is different to the input size (torch.Size([41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/500\n",
      "----------\n",
      "train Loss: 0.0570\n",
      "val Loss: 0.0005\n",
      "\n",
      "Epoch 12/500\n",
      "----------\n",
      "train Loss: 0.0455\n",
      "val Loss: 0.0014\n",
      "\n",
      "Epoch 13/500\n",
      "----------\n",
      "train Loss: 0.0408\n",
      "val Loss: 0.0030\n",
      "\n",
      "Epoch 14/500\n",
      "----------\n",
      "train Loss: 0.0422\n",
      "val Loss: 0.0051\n",
      "\n",
      "Epoch 15/500\n",
      "----------\n",
      "train Loss: 0.0343\n",
      "val Loss: 0.0074\n",
      "\n",
      "Epoch 16/500\n",
      "----------\n",
      "train Loss: 0.0333\n",
      "val Loss: 0.0100\n",
      "\n",
      "Epoch 17/500\n",
      "----------\n",
      "train Loss: 0.0306\n",
      "val Loss: 0.0126\n",
      "\n",
      "Epoch 18/500\n",
      "----------\n",
      "train Loss: 0.0278\n",
      "val Loss: 0.0153\n",
      "\n",
      "Epoch 19/500\n",
      "----------\n",
      "train Loss: 0.0276\n",
      "val Loss: 0.0178\n",
      "\n",
      "Epoch 20/500\n",
      "----------\n",
      "train Loss: 0.0262\n",
      "val Loss: 0.0203\n",
      "\n",
      "Epoch 21/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0225\n",
      "\n",
      "Epoch 22/500\n",
      "----------\n",
      "train Loss: 0.0291\n",
      "val Loss: 0.0247\n",
      "\n",
      "Epoch 23/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0269\n",
      "\n",
      "Epoch 24/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0288\n",
      "\n",
      "Epoch 25/500\n",
      "----------\n",
      "train Loss: 0.0260\n",
      "val Loss: 0.0303\n",
      "\n",
      "Epoch 26/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0320\n",
      "\n",
      "Epoch 27/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0334\n",
      "\n",
      "Epoch 28/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0344\n",
      "\n",
      "Epoch 29/500\n",
      "----------\n",
      "train Loss: 0.0266\n",
      "val Loss: 0.0355\n",
      "\n",
      "Epoch 30/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0365\n",
      "\n",
      "Epoch 31/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0374\n",
      "\n",
      "Epoch 32/500\n",
      "----------\n",
      "train Loss: 0.0266\n",
      "val Loss: 0.0379\n",
      "\n",
      "Epoch 33/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 34/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 35/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 36/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 37/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 38/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 39/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 40/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 41/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 42/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 43/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 44/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 45/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 46/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 47/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 48/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 49/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 50/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 51/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 52/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 53/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 54/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 55/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 56/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 57/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 58/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 59/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 60/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 61/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 62/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 63/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 64/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 65/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 66/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 67/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 68/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 69/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 70/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 71/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 72/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 73/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 74/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 75/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 76/500\n",
      "----------\n",
      "train Loss: 0.0222\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 77/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 78/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 79/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 80/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 81/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 82/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 83/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 84/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 85/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 86/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 87/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 88/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 89/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 90/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 91/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 92/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 93/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 94/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 95/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 96/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 97/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 98/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 99/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 100/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 101/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 102/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 103/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 104/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 105/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 106/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 107/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 108/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 109/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 110/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 111/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 112/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 113/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 114/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 115/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 116/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 117/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 118/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 119/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 120/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 121/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 122/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 123/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 124/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 125/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 126/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 127/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 128/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 129/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 130/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 131/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 132/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 133/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 134/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 135/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 136/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 137/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 138/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 139/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 140/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 141/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 142/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 143/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 144/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 145/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 146/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 147/500\n",
      "----------\n",
      "train Loss: 0.0224\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 148/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 149/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 150/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 151/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 152/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 153/500\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 154/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 155/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 156/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 157/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 158/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 159/500\n",
      "----------\n",
      "train Loss: 0.0270\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 160/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 161/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 162/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 163/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 164/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 165/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 166/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 167/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 168/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 169/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 170/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 171/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 172/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 173/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 174/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 175/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 176/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 177/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 178/500\n",
      "----------\n",
      "train Loss: 0.0266\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 179/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 180/500\n",
      "----------\n",
      "train Loss: 0.0265\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 181/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 182/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 183/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 184/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 185/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 186/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 187/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 188/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 189/500\n",
      "----------\n",
      "train Loss: 0.0257\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 190/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 191/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 192/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 193/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 194/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 195/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 196/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 197/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 198/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 199/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 200/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 201/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 202/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 203/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 204/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 205/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 206/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 207/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 208/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 209/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 210/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 211/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 212/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 213/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 214/500\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 215/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 216/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 217/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 218/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 219/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 220/500\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 221/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 222/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 223/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 224/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 225/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 226/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 227/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 228/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 229/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 230/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 231/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 232/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0382\n",
      "\n",
      "Epoch 233/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 234/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 235/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 236/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 237/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 238/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 239/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 240/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 241/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 242/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 243/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 244/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 245/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0421\n",
      "\n",
      "Epoch 246/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0428\n",
      "\n",
      "Epoch 247/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0433\n",
      "\n",
      "Epoch 248/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0424\n",
      "\n",
      "Epoch 249/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 250/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 251/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 252/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 253/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 254/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 255/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 256/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 257/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 258/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0371\n",
      "\n",
      "Epoch 259/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0374\n",
      "\n",
      "Epoch 260/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0379\n",
      "\n",
      "Epoch 261/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 262/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 263/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 264/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 265/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 266/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 267/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 268/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 269/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 270/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0386\n",
      "\n",
      "Epoch 271/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 272/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 273/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 274/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 275/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 276/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 277/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 278/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 279/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 280/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 281/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 282/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 283/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0422\n",
      "\n",
      "Epoch 284/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 285/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 286/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 287/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 288/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 289/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 290/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 291/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 292/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 293/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 294/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 295/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 296/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 297/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 298/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 299/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 300/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0380\n",
      "\n",
      "Epoch 301/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 302/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 303/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 304/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 305/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 306/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 307/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 308/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 309/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 310/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 311/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 312/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 313/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 314/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 315/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 316/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 317/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 318/500\n",
      "----------\n",
      "train Loss: 0.0223\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 319/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 320/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 321/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 322/500\n",
      "----------\n",
      "train Loss: 0.0258\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 323/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 324/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 325/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 326/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 327/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 328/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 329/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 330/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 331/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 332/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 333/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 334/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 335/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 336/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 337/500\n",
      "----------\n",
      "train Loss: 0.0228\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 338/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 339/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 340/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 341/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0379\n",
      "\n",
      "Epoch 342/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 343/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 344/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 345/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 346/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 347/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 348/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0424\n",
      "\n",
      "Epoch 349/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0423\n",
      "\n",
      "Epoch 350/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0424\n",
      "\n",
      "Epoch 351/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 352/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 353/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 354/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 355/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 356/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 357/500\n",
      "----------\n",
      "train Loss: 0.0223\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 358/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 359/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 360/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 361/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 362/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 363/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 364/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 365/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 366/500\n",
      "----------\n",
      "train Loss: 0.0254\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 367/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 368/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 369/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 370/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 371/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 372/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 373/500\n",
      "----------\n",
      "train Loss: 0.0226\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 374/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 375/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 376/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 377/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 378/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 379/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 380/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0398\n",
      "\n",
      "Epoch 381/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 382/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 383/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 384/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 385/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 386/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 387/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 388/500\n",
      "----------\n",
      "train Loss: 0.0234\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 389/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 390/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 391/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 392/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 393/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 394/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0413\n",
      "\n",
      "Epoch 395/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 396/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 397/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 398/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 399/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 400/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0409\n",
      "\n",
      "Epoch 401/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 402/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0427\n",
      "\n",
      "Epoch 403/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0421\n",
      "\n",
      "Epoch 404/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0414\n",
      "\n",
      "Epoch 405/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 406/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 407/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 408/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 409/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 410/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0384\n",
      "\n",
      "Epoch 411/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 412/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 413/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0411\n",
      "\n",
      "Epoch 414/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0406\n",
      "\n",
      "Epoch 415/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 416/500\n",
      "----------\n",
      "train Loss: 0.0259\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 417/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 418/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 419/500\n",
      "----------\n",
      "train Loss: 0.0261\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 420/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 421/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0417\n",
      "\n",
      "Epoch 422/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 423/500\n",
      "----------\n",
      "train Loss: 0.0255\n",
      "val Loss: 0.0408\n",
      "\n",
      "Epoch 424/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0416\n",
      "\n",
      "Epoch 425/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0418\n",
      "\n",
      "Epoch 426/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 427/500\n",
      "----------\n",
      "train Loss: 0.0256\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 428/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0402\n",
      "\n",
      "Epoch 429/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 430/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0404\n",
      "\n",
      "Epoch 431/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0390\n",
      "\n",
      "Epoch 432/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0385\n",
      "\n",
      "Epoch 433/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0381\n",
      "\n",
      "Epoch 434/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 435/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 436/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 437/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 438/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 439/500\n",
      "----------\n",
      "train Loss: 0.0231\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 440/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 441/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 442/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 443/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 444/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 445/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 446/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 447/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 448/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 449/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0403\n",
      "\n",
      "Epoch 450/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 451/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0405\n",
      "\n",
      "Epoch 452/500\n",
      "----------\n",
      "train Loss: 0.0248\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 453/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0419\n",
      "\n",
      "Epoch 454/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0431\n",
      "\n",
      "Epoch 455/500\n",
      "----------\n",
      "train Loss: 0.0245\n",
      "val Loss: 0.0434\n",
      "\n",
      "Epoch 456/500\n",
      "----------\n",
      "train Loss: 0.0230\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 457/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0388\n",
      "\n",
      "Epoch 458/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0374\n",
      "\n",
      "Epoch 459/500\n",
      "----------\n",
      "train Loss: 0.0227\n",
      "val Loss: 0.0377\n",
      "\n",
      "Epoch 460/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0377\n",
      "\n",
      "Epoch 461/500\n",
      "----------\n",
      "train Loss: 0.0243\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 462/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 463/500\n",
      "----------\n",
      "train Loss: 0.0239\n",
      "val Loss: 0.0421\n",
      "\n",
      "Epoch 464/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 465/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 466/500\n",
      "----------\n",
      "train Loss: 0.0233\n",
      "val Loss: 0.0396\n",
      "\n",
      "Epoch 467/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0383\n",
      "\n",
      "Epoch 468/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0387\n",
      "\n",
      "Epoch 469/500\n",
      "----------\n",
      "train Loss: 0.0247\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 470/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0412\n",
      "\n",
      "Epoch 471/500\n",
      "----------\n",
      "train Loss: 0.0229\n",
      "val Loss: 0.0422\n",
      "\n",
      "Epoch 472/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0420\n",
      "\n",
      "Epoch 473/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0431\n",
      "\n",
      "Epoch 474/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0426\n",
      "\n",
      "Epoch 475/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0410\n",
      "\n",
      "Epoch 476/500\n",
      "----------\n",
      "train Loss: 0.0269\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 477/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 478/500\n",
      "----------\n",
      "train Loss: 0.0238\n",
      "val Loss: 0.0391\n",
      "\n",
      "Epoch 479/500\n",
      "----------\n",
      "train Loss: 0.0246\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 480/500\n",
      "----------\n",
      "train Loss: 0.0253\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 481/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 482/500\n",
      "----------\n",
      "train Loss: 0.0237\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 483/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0393\n",
      "\n",
      "Epoch 484/500\n",
      "----------\n",
      "train Loss: 0.0244\n",
      "val Loss: 0.0382\n",
      "\n",
      "Epoch 485/500\n",
      "----------\n",
      "train Loss: 0.0240\n",
      "val Loss: 0.0389\n",
      "\n",
      "Epoch 486/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0397\n",
      "\n",
      "Epoch 487/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0401\n",
      "\n",
      "Epoch 488/500\n",
      "----------\n",
      "train Loss: 0.0249\n",
      "val Loss: 0.0399\n",
      "\n",
      "Epoch 489/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0395\n",
      "\n",
      "Epoch 490/500\n",
      "----------\n",
      "train Loss: 0.0241\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 491/500\n",
      "----------\n",
      "train Loss: 0.0235\n",
      "val Loss: 0.0400\n",
      "\n",
      "Epoch 492/500\n",
      "----------\n",
      "train Loss: 0.0232\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 493/500\n",
      "----------\n",
      "train Loss: 0.0250\n",
      "val Loss: 0.0392\n",
      "\n",
      "Epoch 494/500\n",
      "----------\n",
      "train Loss: 0.0236\n",
      "val Loss: 0.0394\n",
      "\n",
      "Epoch 495/500\n",
      "----------\n",
      "train Loss: 0.0252\n",
      "val Loss: 0.0407\n",
      "\n",
      "Epoch 496/500\n",
      "----------\n",
      "train Loss: 0.0263\n",
      "val Loss: 0.0429\n",
      "\n",
      "Epoch 497/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0450\n",
      "\n",
      "Epoch 498/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0448\n",
      "\n",
      "Epoch 499/500\n",
      "----------\n",
      "train Loss: 0.0242\n",
      "val Loss: 0.0415\n",
      "\n",
      "Epoch 500/500\n",
      "----------\n",
      "train Loss: 0.0251\n",
      "val Loss: 0.0392\n",
      "\n",
      "Training complete in 0m 14s\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model, val_acc_history = train_model(model, dataloaders_dict, criterion, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84d5195e-5625-42b9-ab2d-f2793a04a58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_1D(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv1d(2, 8, kernel_size=(4,), stride=(1,))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv1d(8, 16, kernel_size=(2,), stride=(1,))\n",
       "    (1): Tanh()\n",
       "    (2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "  )\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e0cd68e-271e-4c7d-ba0d-1cf3ef256c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model,criterion, test_loader):\n",
    "    model.eval()   # 모델을 validation mode로 설정\n",
    "    \n",
    "    # test_loader에 대하여 검증 진행 (gradient update 방지)\n",
    "    with torch.no_grad():\n",
    "        #corrects = 0\n",
    "        total = 0\n",
    "        label_list = []\n",
    "        output_list = [] \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # forward\n",
    "            # input을 model에 넣어 output을 도출\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            outputs = outputs.detach().cpu().numpy().tolist()\n",
    "            output_list.extend(outputs)\n",
    "            label_list.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "            \n",
    "    return output_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46fd6e15-493d-496b-9d99-c8966172cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list,label_list = test_model(model, criterion, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d4ac77b-8f41-48b9-b8f6-77b16f87eb6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.4377591609954834],\n",
       " [2.4377570152282715],\n",
       " [2.4377377033233643],\n",
       " [2.437736749649048],\n",
       " [2.4377338886260986],\n",
       " [2.437741994857788],\n",
       " [2.4377505779266357],\n",
       " [2.4377567768096924],\n",
       " [2.4377601146698],\n",
       " [2.4377357959747314],\n",
       " [2.4377167224884033],\n",
       " [2.437703847885132],\n",
       " [2.4377050399780273],\n",
       " [2.4377243518829346],\n",
       " [2.437735080718994],\n",
       " [2.4377431869506836],\n",
       " [2.437739133834839],\n",
       " [2.437736988067627],\n",
       " [2.4377424716949463],\n",
       " [2.4377400875091553],\n",
       " [2.437736749649048],\n",
       " [2.437732458114624],\n",
       " [2.437729597091675],\n",
       " [2.4377307891845703],\n",
       " [2.437734365463257],\n",
       " [2.4377377033233643],\n",
       " [2.437741994857788],\n",
       " [2.4377493858337402],\n",
       " [2.4377543926239014],\n",
       " [2.4377593994140625],\n",
       " [2.4377520084381104],\n",
       " [2.437731981277466],\n",
       " [2.4377291202545166],\n",
       " [2.4377222061157227],\n",
       " [2.4377329349517822],\n",
       " [2.4377472400665283],\n",
       " [2.437736749649048],\n",
       " [2.437732458114624],\n",
       " [2.4377434253692627],\n",
       " [2.437751531600952],\n",
       " [2.4377620220184326],\n",
       " [2.4377729892730713]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7074db2d-79e8-4ad4-ab32-1edbba2b7f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.304606,\n",
       " 2.396678,\n",
       " 2.466772,\n",
       " 2.3624077,\n",
       " 2.3153226,\n",
       " 2.307558,\n",
       " 2.4113579,\n",
       " 2.4476678,\n",
       " 2.4475827,\n",
       " 2.2919137,\n",
       " 2.2839155,\n",
       " 2.30857,\n",
       " 2.431437,\n",
       " 2.3871229,\n",
       " 2.3663397,\n",
       " 2.4650793,\n",
       " 2.4962463,\n",
       " 2.5044162,\n",
       " 2.4053051,\n",
       " 2.4317226,\n",
       " 2.384631,\n",
       " 2.4047058,\n",
       " 2.3639357,\n",
       " 2.4228513,\n",
       " 2.5234191,\n",
       " 2.349637,\n",
       " 2.430969,\n",
       " 2.4822721,\n",
       " 2.47304,\n",
       " 2.435214,\n",
       " 2.3145738,\n",
       " 2.440728,\n",
       " 2.2672696,\n",
       " 2.2844753,\n",
       " 2.4609153,\n",
       " 2.5124917,\n",
       " 2.4430888,\n",
       " 2.5957258,\n",
       " 2.4064314,\n",
       " 2.1980708,\n",
       " 2.4664056,\n",
       " 2.375425]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d91c535b-7dcd-41c8-b538-db3e900b2f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25626253370>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(output_list)\n",
    "plt.plot(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4399198d-15e7-4aa5-adcb-202927f07572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1c6a8b7-732f-4ca8-b268-0a8820b427cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06752790155864898"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(output_list,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "151eeef1-f677-4533-be8a-9f34e6710b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0078021113414330386"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(output_list,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18a021-e3d7-47f3-bdf5-46110f596505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f463938a-1865-4e6c-85c0-d1b8187c6cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
